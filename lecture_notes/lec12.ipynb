{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0002 Lecture 12: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speaker:** Prof. John Guttag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Paradigm\n",
    "- observe set of examples: **training data**\n",
    "- infer something about process that generated that data\n",
    "- use inference to make predictions about previously unseen data: **test data**\n",
    "- *supervised*: given a set of feature/label pairs, find a rule that predicts the label associated with a previously unseen input\n",
    "- *unsupervised*: given a set of feature vectors (without labels), group them into \"natural clusters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering is an optimization problem\n",
    "$$\\textrm{variability}(c) = \\sum_{e\\in c}\\textrm{distance}(\\textrm{mean}(c), e)^2$$\n",
    "$$\\textrm{dissimilarity}(C) = \\sum_{c \\in C} \\textrm{variability}(c)$$\n",
    "- why not divide variability by size of cluster?\n",
    "    - big and bad worse than small and bad\n",
    "- is optimization problem finding a $C$ that minimizes dissimilarity$(C)$?\n",
    "    - no, otherwise could put each example in its own cluster\n",
    "- need a constraint, e.g.\n",
    "    - minimum distance between clusters\n",
    "    - number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two popular methods\n",
    "- hierarchical clustering\n",
    "- K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "- 1.) start by assigning each item to a cluster, so that if you have $N$ items, you now have $N$ clusters, each containing just one item.\n",
    "- 2.) find the closest (most similar) pair of clusters and merge them into a single cluster, so that now you have one fewer cluster\n",
    "- 3.) continue the proces until all items are clustered into a single cluster of size $N$\n",
    "- this is called **agglomerative** clustering\n",
    "- what does distance mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linkage metrics\n",
    "- *single-linkage*: consider the distance between one cluster and another cluster to be equal to the **shortest** distance from any member of one cluster to any member of the other cluster\n",
    "- *complete-linkage*: consider the distance between one cluster and another cluster to be equal to the **greatest** distance from any member of one cluster to any member of the other cluster\n",
    "- *average-linkage*: consider the distance between one cluster and another cluster to be equal to the **average** distance from any member of one cluster to any member of the other cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms\n",
    "- hierarchical clustering\n",
    "    - greedy algorithm\n",
    "        - makes locally optimal decisions at each point\n",
    "    - can select number of clusters using dendogram\n",
    "    - deterministic\n",
    "    - flexible with respect to linkage criteria\n",
    "    - slow\n",
    "        - naive algorithm $n^3$\n",
    "        - $n^2$ algorithms exist for some linkage criteria\n",
    "- K-means is a much faster greedy algorithm\n",
    "    - most useful when you know how many clusters you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means algorithm\n",
    "- pseudocode:\n",
    "    - randomly choose k examples as initial centroids\n",
    "    - while True:\n",
    "        - create k clusters by assigning each example to closest centroid\n",
    "        - compute k new centroids by averaging examples in each cluster\n",
    "        - if centroids don't change:\n",
    "            - break\n",
    "- what is complexity of one iteration?\n",
    "    - $k\\cdot n \\cdot d$, where $n$ is the number of points and $d$ time required to compute the distance between a pair of points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with k-means\n",
    "- choosing the \"wrong\" k can lead to strange results\n",
    "- result can depend upon initial centroids\n",
    "    - number of iterations\n",
    "    - even final results\n",
    "        - greedy algorithm can find different local optimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose K\n",
    "- *a priori* knowledge about application domain\n",
    "    - there are two kinds of people in the world: $k=2$\n",
    "    - there are five different types of bacteria: $k=5$\n",
    "- search for a good $k$\n",
    "    - try different values of $k$ and evaluate quality of results\n",
    "    - run hierarchical clustering on subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigating dependence on initial centroids\n",
    "- try multiple sets of randomly chosen initial centroids\n",
    "- select \"best\" result\n",
    "- pseudocode:\n",
    "    - best = kMeans(points)\n",
    "    - for t in range(numTrials):\n",
    "        - C = kMeans(points)\n",
    "        - if dissimilarity(C) < dissimilarity(best):\n",
    "            - best = C\n",
    "    - return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "- many patients with 4 features each\n",
    "    - heart rate in beats per minute\n",
    "    - number of past heart attacks\n",
    "    - age\n",
    "    - ST elevation (binary)\n",
    "- outcome (death) based on features\n",
    "    - probabilistic, not deterministic\n",
    "    - e.g. older people with multiple heart attacks at higher risk\n",
    "- cluster, and examine **purity** of clusters relative to outcomes\n",
    "    - enriched by people who died?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parameters\n",
    "\n",
    "#set line width\n",
    "pylab.rcParams['lines.linewidth'] = 6\n",
    "#set general font size \n",
    "pylab.rcParams['font.size'] = 12\n",
    "#set font size for labels on axes\n",
    "pylab.rcParams['axes.labelsize'] = 18\n",
    "#set size of numbers on x-axis\n",
    "pylab.rcParams['xtick.major.size'] = 5\n",
    "#set size of numbers on y-axis\n",
    "pylab.rcParams['ytick.major.size'] = 5\n",
    "#set size of markers\n",
    "pylab.rcParams['lines.markersize'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minkowski distance\n",
    "def minkowskiDist(v1, v2, p):\n",
    "    # assumes v1 and v2 are equal length arrays of numbers\n",
    "    dist = 0\n",
    "    for i in range(len(v1)):\n",
    "        dist += abs(v1[i] - v2[i])**p\n",
    "    return dist**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    def __init__(self, name, features, label=None):\n",
    "        # Assumes features is an array of floats\n",
    "        self.name = name\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "    \n",
    "    def dimensionality(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def getFeatures(self):\n",
    "        return self.features[:]\n",
    "\n",
    "    def getLabel(self):\n",
    "        return self.label\n",
    "    \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    \n",
    "    def distance(self, other):\n",
    "        return minkowskiDist(self.features, other.getFeatures(), 2)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self,name + ':' + str(self.features) + ':'\\\n",
    "                + str(self.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster(object):\n",
    "    def __init__(self, examples):\n",
    "        \"\"\"Assumes examples a non-empty list of Examples\"\"\"\n",
    "        self.examples = examples\n",
    "        self.centroid = self.computeCentroid()\n",
    "    \n",
    "    def update(self, examples):\n",
    "        \"\"\"Assumes examples is a non-empty list of Examples\n",
    "            Replace examples; return amount centroid has changed\"\"\"\n",
    "        oldCentroid = self.centroid\n",
    "        self.examples = examples\n",
    "        self.centroid = self.computeCentroid()\n",
    "        return oldCentroid.distance(self.centroid)\n",
    "    \n",
    "    def computeCentroid(self):\n",
    "        vals = pylab.array([0.0]*self.examples[0].dimensionality())\n",
    "        for e in self.examples:  # compute mean\n",
    "            vals += e.getFeatures()\n",
    "        centroid = Example('centroid', vals/len(self.examples))\n",
    "        return centroid\n",
    "\n",
    "    def getCentroid(self):\n",
    "        return self.centroid\n",
    "    \n",
    "    def variability(self):\n",
    "        totDist = 0\n",
    "        for e in self.examples:\n",
    "            totDist += (e.distance(self.centroid))**2\n",
    "        return totDist\n",
    "    \n",
    "    def members(self):\n",
    "        for e in self.examples:\n",
    "            yield e\n",
    "    \n",
    "    def __str__(self):\n",
    "        names = []\n",
    "        for e in self.examples:\n",
    "            names.append(e.getName)\n",
    "        names.sort()\n",
    "        result = 'Cluster with centroid'\\\n",
    "                + str(self.centroid.getFeatures()) + ' contains:\\n '\n",
    "        for e in names: \n",
    "            result = result + e + ', '\n",
    "        return result[:-2]  # remove trailing comma and space\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating a clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity(clusters):\n",
    "    \"\"\" Assumes clusters a list of clusters\n",
    "        Returns a measure of the total dissimilarity of the\n",
    "        clusters in the list\"\"\"\n",
    "    totDist = 0\n",
    "    for c in clusters:\n",
    "        totDist += c.variability()\n",
    "    return totDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient(Example):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scaling\n",
    "def scaleAttrs(vals):\n",
    "    vals = pylab.array(vals)\n",
    "    mean = sum(vals) / len(vals)\n",
    "    sd = numpy.std(vals)\n",
    "    vals = vals - mean\n",
    "    return vals/sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(toScale=False):\n",
    "    # read in data\n",
    "    hrList, stElevList, ageList, prevACSList, classList = [],[],[],[],[]\n",
    "    cardiacData = open('cardiacData.txt', 'r')\n",
    "    for l in cardiacData:\n",
    "        l = l.split(',')\n",
    "        hrList.append(int(l[0]))\n",
    "        stElevList.append(int(l[1]))\n",
    "        ageList.append(int(l[2]))\n",
    "        prevACSList.append(int(l[3]))\n",
    "        classList.append(int(l[4]))\n",
    "    if toScale:\n",
    "        hrList = scaleAttrs(hrList)\n",
    "        stElevList = scaleAttrs(stElevList)\n",
    "        ageList = scaleAttrs(ageList)\n",
    "        prevACSList = scaleAttrs(prevACSList)\n",
    "    #Build points\n",
    "    points = []\n",
    "    for i in range(len(hrList)):\n",
    "        features = pylab.array([hrList[i], prevACSList[i],\\\n",
    "                                stElevList[i], ageList[i]])\n",
    "        pIndex = str(i)\n",
    "        points.append(Patient('P'+ pIndex, features, classList[i]))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(examples, k, verbose=False):\n",
    "    # get k randomly chosen initial centroids,\n",
    "    # create cluster for each\n",
    "    initialCentroids = random.sample(examples, k)\n",
    "    clusters = []\n",
    "    for e in initialCentroids:\n",
    "        clusters.append(Cluster([e]))\n",
    "    \n",
    "    # iterate until centroids do not change\n",
    "    converged = False\n",
    "    numIterations = 0\n",
    "    while not converged:\n",
    "        numIterations += 1\n",
    "        # create a list containing k distinct empty lists\n",
    "        newClusters = []\n",
    "        for i in range(k):\n",
    "            newClusters.append([])\n",
    "        \n",
    "        # associate each example with closest centroid\n",
    "        for e in examples:\n",
    "            # find the centroid closest to e\n",
    "            smallestDistance = e.distance(clusters[0].getCentroid())\n",
    "            index = 0\n",
    "            for i in range(1, k):\n",
    "                distance = e.distance(clusters[i].getCentroid())\n",
    "                if distance < smallestDistance:\n",
    "                    smallestDistance = distance\n",
    "                    index = i\n",
    "            # add e to the list of examples for appropriate cluster\n",
    "            newClusters[index].append(e)\n",
    "        \n",
    "        for c in newClusters:  # avoid having empty clusters\n",
    "            if len(c) == 0:\n",
    "                raise ValueError('Empty Cluster')\n",
    "        \n",
    "        # update each cluster; check if a centroid has changed\n",
    "        converged = True\n",
    "        for i in range(k):\n",
    "            if clusters[i].update(newClusters[i]) > 0.0:\n",
    "                converged = False\n",
    "        if verbose:\n",
    "            print('Itertaion #' + str(numIterations))\n",
    "            for c in clusters:\n",
    "                print(c)\n",
    "            print('')  # add blank line\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trykmeans(examples, numClusters, numTrials, verbose = False):\n",
    "    \"\"\"Calls kmeans numTrials times and returns the result with the\n",
    "          lowest dissimilarity\"\"\"\n",
    "    best = kmeans(examples, numClusters, verbose)\n",
    "    minDissimilarity = dissimilarity(best)\n",
    "    trial = 1\n",
    "    while trial < numTrials:\n",
    "        try:\n",
    "            clusters = kmeans(examples, numClusters, verbose)\n",
    "        except ValueError:\n",
    "            continue #If failed, try again\n",
    "        currDissimilarity = dissimilarity(clusters)\n",
    "        if currDissimilarity < minDissimilarity:\n",
    "            best = clusters\n",
    "            minDissimilarity = currDissimilarity\n",
    "        trial += 1\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClustering(clustering):\n",
    "    \"\"\"Assumes: clustering is a sequence of clusters\n",
    "       Prints information about each cluster\n",
    "       Returns list of fraction of pos cases in each cluster\"\"\"\n",
    "    posFracs = []\n",
    "    for c in clustering:\n",
    "        numPts = 0\n",
    "        numPos = 0\n",
    "        for p in c.members():\n",
    "            numPts += 1\n",
    "            if p.getLabel() == 1:\n",
    "                numPos += 1\n",
    "        fracPos = numPos/numPts\n",
    "        posFracs.append(fracPos)\n",
    "        print('Cluster of size', numPts, 'with fraction of positives =',\n",
    "              round(fracPos, 4))\n",
    "    return pylab.array(posFracs)\n",
    "\n",
    "def testClustering(patients, numClusters, seed = 0, numTrials = 5):\n",
    "    random.seed(seed)\n",
    "    bestClustering = trykmeans(patients, numClusters, numTrials)\n",
    "    posFracs = printClustering(bestClustering)\n",
    "    return posFracs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Test k-means (k = 2)\n",
      "Cluster of size 118 with fraction of positives = 0.3305\n",
      "Cluster of size 132 with fraction of positives = 0.3333\n"
     ]
    }
   ],
   "source": [
    "patients = getData()\n",
    "for k in (2,):\n",
    "    print('\\n     Test k-means (k = ' + str(k) + ')')\n",
    "    posFracs = testClustering(patients, k, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Test k-means (k = 2)\n",
      "Cluster of size 224 with fraction of positives = 0.2902\n",
      "Cluster of size 26 with fraction of positives = 0.6923\n"
     ]
    }
   ],
   "source": [
    "# now use toScale\n",
    "patients = getData(True)\n",
    "for k in (2,):\n",
    "    print('\\n     Test k-means (k = ' + str(k) + ')')\n",
    "    posFracs = testClustering(patients, k, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many positives are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positive patients = 83\n"
     ]
    }
   ],
   "source": [
    "numPos = 0\n",
    "for p in patients:\n",
    "    if p.getLabel() == 1:\n",
    "        numPos += 1\n",
    "print('Total number of positive patients =', numPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A hypothesis\n",
    "- different subgroups of positive patients have different characteristics\n",
    "- how might we test this?\n",
    "- try some other values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Test k-means (k = 2)\n",
      "Cluster of size 118 with fraction of positives = 0.3305\n",
      "Cluster of size 132 with fraction of positives = 0.3333\n",
      "\n",
      "     Test k-means (k = 4)\n",
      "Cluster of size 53 with fraction of positives = 0.2642\n",
      "Cluster of size 85 with fraction of positives = 0.3529\n",
      "Cluster of size 41 with fraction of positives = 0.3902\n",
      "Cluster of size 71 with fraction of positives = 0.3239\n",
      "\n",
      "     Test k-means (k = 6)\n",
      "Cluster of size 41 with fraction of positives = 0.3902\n",
      "Cluster of size 38 with fraction of positives = 0.2105\n",
      "Cluster of size 38 with fraction of positives = 0.4211\n",
      "Cluster of size 42 with fraction of positives = 0.381\n",
      "Cluster of size 27 with fraction of positives = 0.3333\n",
      "Cluster of size 64 with fraction of positives = 0.2812\n"
     ]
    }
   ],
   "source": [
    "patients = getData()\n",
    "for k in (2, 4, 6):\n",
    "    print('\\n     Test k-means (k = ' + str(k) + ')')\n",
    "    posFracs = testClustering(patients, k, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
